{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = pd.read_csv('/Users/nickmac/Desktop/Dev Stuff/Kaggle/Black Friday/train.csv', index_col='User_ID')\n",
    "\n",
    "X_test = pd.read_csv('/Users/nickmac/Desktop/Dev Stuff/Kaggle/Black Friday/test.csv', index_col='User_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns with missing target\n",
    "\n",
    "X_full.dropna(axis=0, subset=['Purchase'], inplace=True)\n",
    "y = X_full['Purchase']\n",
    "X_full.drop(['Purchase'], axis=1, inplace=True)\n",
    "\n",
    "# Break off validation set from training data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_full, y, train_size=0.8, test_size=0.2,\n",
    "                                                      random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical and categorical columns\n",
    "numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Model with hyperparameter search space\n",
    "xgb_model = XGBRegressor(random_state=42)\n",
    "\n",
    "\n",
    "param_distributions = {\n",
    "    'regressor__n_estimators': [50, 100, 200, 500],  # Number of trees\n",
    "    'regressor__learning_rate': [0.01, 0.1, 0.2],   # Step size shrinkage\n",
    "    'regressor__max_depth': [3, 5, 7],                # Maximum tree depth\n",
    "    'regressor__subsample': [0.8, 0.9, 1.0],         # Fraction of samples used for fitting each tree\n",
    "    'regressor__colsample_bytree': [0.8, 0.9, 1.0]   # Fraction of features used for fitting each tree\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    }
   ],
   "source": [
    "# Pipeline with Randomized Search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', xgb_model)\n",
    "    ]),\n",
    "    param_distributions=param_distributions, \n",
    "    n_iter=100,  # Number of parameter combinations to try\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=5,  \n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found by random search:\", random_search.best_params_)\n",
    "print(\"Best MAE found by random search:\", -random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search (Refine around best parameters from random search)\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [random_search.best_params_['regressor__n_estimators'] - 25, \n",
    "                                random_search.best_params_['regressor__n_estimators'], \n",
    "                                random_search.best_params_['regressor__n_estimators'] + 25],\n",
    "    'regressor__learning_rate': [random_search.best_params_['regressor__learning_rate'] - 0.05, \n",
    "                                 random_search.best_params_['regressor__learning_rate'], \n",
    "                                 random_search.best_params_['regressor__learning_rate'] + 0.05],\n",
    "    'regressor__max_depth': [random_search.best_params_['regressor__max_depth'] - 1,\n",
    "                            random_search.best_params_['regressor__max_depth'],\n",
    "                            random_search.best_params_['regressor__max_depth'] + 1]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=random_search.best_estimator_, param_grid=param_grid, scoring='neg_mean_absolute_error', cv=5, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model from grid search and make predictions\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best parameters found by grid search:\", grid_search.best_params_)\n",
    "print(\"Best MAE found by grid search:\", -grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = best_model.predict(X_valid)\n",
    "\n",
    "# Calculate MAE on the validation set\n",
    "mae_valid = mean_absolute_error(y_valid, preds)\n",
    "print(f\"Mean Absolute Error (Validation Set): {mae_valid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert processed data back to DataFrames\n",
    "# X_train_processed = pd.DataFrame(X_train_processed, columns=preprocessor.get_feature_names_out())\n",
    "# X_valid_processed = pd.DataFrame(X_valid_processed, columns=preprocessor.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit and transform training data\n",
    "# X_train_processed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# # Transform validation data (using the same preprocessor)\n",
    "# X_valid_processed = preprocessor.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # XGBoost Model\n",
    "# xgb_model = XGBRegressor(n_estimators=500, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# # Create the full pipeline\n",
    "# full_pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('regressor', xgb_model) \n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit the pipeline (includes preprocessing and model training)\n",
    "# full_pipeline.fit(X_train, y_train, regressor__early_stopping_rounds=5, regressor__eval_set=[(X_valid, y_valid)], regressor__verbose=False)\n",
    "\n",
    "\n",
    "# # Get predictions on the validation set\n",
    "# preds = full_pipeline.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Separate numerical and categorical columns\n",
    "# X_train_num = X_train.select_dtypes(exclude='object')\n",
    "# X_train_cat = X_train.select_dtypes(include='object')\n",
    "\n",
    "# X_valid_num = X_valid.select_dtypes(exclude='object')\n",
    "# X_valid_cat = X_valid.select_dtypes(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Numerical Imputation (Mean)\n",
    "# num_imputer = SimpleImputer(strategy='mean')  \n",
    "# imputed_X_train_num = pd.DataFrame(num_imputer.fit_transform(X_train_num))\n",
    "# imputed_X_valid_num = pd.DataFrame(num_imputer.transform(X_valid_num))\n",
    "\n",
    "# # Reassign column names after imputation\n",
    "# imputed_X_train_num.columns = X_train_num.columns\n",
    "# imputed_X_valid_num.columns = X_valid_num.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2. Categorical Imputation (Most Frequent)\n",
    "# cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "# imputed_X_train_cat = pd.DataFrame(cat_imputer.fit_transform(X_train_cat))\n",
    "# imputed_X_valid_cat = pd.DataFrame(cat_imputer.transform(X_valid_cat))\n",
    "\n",
    "# # Reassign column names after imputation\n",
    "# imputed_X_train_cat.columns = X_train_cat.columns\n",
    "# imputed_X_valid_cat.columns = X_valid_cat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # One-Hot Encoding\n",
    "# OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "# OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(imputed_X_train_cat))\n",
    "# OH_cols_valid = pd.DataFrame(OH_encoder.transform(imputed_X_valid_cat))\n",
    "\n",
    "# # Restore index after encoding\n",
    "# OH_cols_train.index = imputed_X_train_cat.index\n",
    "# OH_cols_valid.index = imputed_X_valid_cat.index\n",
    "\n",
    "# # Ensure all columns have string type\n",
    "# OH_cols_train.columns = OH_cols_train.columns.astype(str)\n",
    "# OH_cols_valid.columns = OH_cols_valid.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine numerical and OH encoded data\n",
    "# final_X_train = pd.concat([imputed_X_train_num, OH_cols_train], axis=1)\n",
    "# final_X_valid = pd.concat([imputed_X_valid_num, OH_cols_valid], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do\n",
    "\n",
    "- Get all numerical and categorical variables\n",
    "- Impute numerical variables and replace categorical with most frequent strategy\n",
    "- You also need to process the test data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
